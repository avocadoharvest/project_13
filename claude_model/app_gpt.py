import gradio as gr
import os
import tempfile
from dotenv import load_dotenv
from gtts import gTTS

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate

from text2speech import speech_to_text  # ë°˜ë“œì‹œ íŒŒì¼ë¡œë¶€í„°(stt) ì¸ì‹í•˜ëŠ” êµ¬ì¡°ë¡œ ë˜ì–´ ìˆì–´ì•¼ í•¨

# ğŸš© NEW: OpenAI ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
import openai
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain

load_dotenv()

# ğŸš© NEW: OpenAI API í‚¤ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
openai.api_key = os.getenv("OPENAI_API_KEY")

vectorstore = None
questions = []
question_index = 0
evaluate_chain = None

def load_vectorstore():
    global vectorstore
    db_path = "./db/faiss"
    if not os.path.exists(db_path):
        return "âŒ ë²¡í„°ìŠ¤í† ì–´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    return "âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì™„ë£Œ!"

def extract_questions(pdf_file):
    global questions, question_index
    if pdf_file is None:
        return "âŒ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
    pdf_path = pdf_file.name
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, separators=["\n\n", "\n", ".", "?"])
    docs = splitter.split_documents(pages)

    extracted = []
    for doc in docs:
        lines = doc.page_content.strip().split("\n")
        for line in lines:
            if line.strip().startswith("Q") and "?" in line:
                extracted.append(line.strip())
    questions = extracted
    question_index = 0
    return f"âœ… ì´ {len(questions)}ê°œì˜ ë¬¸ì œê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤."

def setup_chain():
    global evaluate_chain, vectorstore
    if vectorstore is None:
        return "â— ë¨¼ì € ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”."
    retriever = vectorstore.as_retriever()

    # ğŸš© PromptTemplate ë™ì¼í•˜ê²Œ ì‚¬ìš© 
    prompt = PromptTemplate.from_template("""
ë„ˆëŠ” ì˜ë¬¸ ë…í•´ ì‹œí—˜ì˜ í‰ê°€ìì•¼. ì•„ë˜ Q(ë¬¸ì œ)ì™€ A(ìˆ˜í—˜ìƒ ë‹µë³€)ê°€ ìˆì„ ë•Œ,
#Context: (ë¬¸ì„œì—ì„œ ë½‘ì€ ì •ë³´), #Question: (ë¬¸ì œ), #Answer: (ë°›ì•„ì“´ ë‹µë³€)

ìˆ˜í—˜ìƒì´ ì •ë‹µì— ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€, ë…¼ë¦¬ì  ê·¼ê±°ê°€ ì •í™•í•œì§€ 'contextë§Œ í™œìš©'í•´ì„œ í•œê¸€ë¡œ 5ë¬¸ì¥ìœ¼ë¡œ í‰ê°€, ì ìˆ˜(A~Eì , ì†Œìˆ˜ì  ê°€ëŠ¥)ë¡œ ë§¤ê²¨ì¤˜.
í”¼ë“œë°±ì€ í•œê¸€ë¡œ ë¶€íƒí•´.

#Question: {question}
#Answer: {answer}
#Context: {context}

[ì¶œë ¥ ì˜ˆì‹œ]   
ì ìˆ˜: Bì    
í”¼ë“œë°±: ë‹µë³€ì´ í•µì‹¬ ë‚´ìš©ì„ ì˜ ì–¸ê¸‰í–ˆìœ¼ë‚˜, ì„¸ë¶€ì •ë³´ê°€ ë¹ ì§.
""")

    # ğŸš© ë‹¬ë¼ì§€ëŠ” ë¶€ë¶„: OpenAI(4o) ê¸°ë°˜ LLM ë° Chain ì„¸íŒ…
    llm = ChatOpenAI(
        model="gpt-4o",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        temperature=0,
        streaming=False,
    )
    evaluate_chain = LLMChain(
        llm=llm,
        prompt=prompt,
        # context is set at evaluation-time below
    )
    return "âœ… í‰ê°€ ì²´ì¸ ì¤€ë¹„ ì™„ë£Œ!"

def get_next_question():
    global question_index
    if not questions:
        return "â— ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", None
    if question_index >= len(questions):
        return "ğŸ‰ ëª¨ë“  ë¬¸ì œë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!", None

    question = questions[question_index]
    question_index += 1

    # í…ìŠ¤íŠ¸, mp3 íŒŒì¼ ê²½ë¡œ ë°˜í™˜
    tts = gTTS(text=question, lang='en')
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(tmp.name)
    tmp.close()
    return f"[ë¬¸ì œ {question_index}] {question}", tmp.name

def evaluate_audio_response(audio):
    global question_index, evaluate_chain
    if evaluate_chain is None:
        return "âŒ í‰ê°€ ì²´ì¸ì„ ë¨¼ì € ì¤€ë¹„í•´ì£¼ì„¸ìš”."
    if question_index == 0:
        return "â— ë¬¸ì œë¥¼ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”."

    question = questions[question_index - 1]
    user_answer = speech_to_text(audio)  # audioëŠ” íŒŒì¼ê²½ë¡œ(str): ì´ê²Œ íŒŒì¼ì—ì„œ STTí•˜ëŠ” í•¨ìˆ˜ì—¬ì•¼ í•¨

    # ğŸš© context ê²€ìƒ‰
    docs = vectorstore.similarity_search(question, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # ğŸš© gpt-4oë¡œ í‰ê°€
    result = evaluate_chain.invoke({
        "question": question,
        "answer": user_answer,
        "context": context,   # context ì¶”ê°€
    })
    return f"ğŸ§  AI í‰ê°€ ê²°ê³¼:\n\n{result['text'] if isinstance(result, dict) and 'text' in result else result}"

with gr.Blocks() as demo:
    gr.Markdown("## ğŸ—£ï¸ í† ìµìŠ¤í”¼í‚¹ AI ì±„ì  ì‹œìŠ¤í…œ")
    with gr.Row():
        pdf_input = gr.File(label="ğŸ“„ ë¬¸ì œ PDF ì—…ë¡œë“œ", file_types=[".pdf"])
        upload_result = gr.Textbox(label="ë¬¸ì œ ì¶”ì¶œ ê²°ê³¼")
    with gr.Row():
        load_btn = gr.Button("ğŸ“‚ ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°")
        vectorstore_status = gr.Textbox(label="ë²¡í„°ìŠ¤í† ì–´ ìƒíƒœ")
    with gr.Row():
        setup_btn = gr.Button("ğŸ§  í‰ê°€ ì²´ì¸ ì¤€ë¹„")
        chain_status = gr.Textbox(label="í‰ê°€ ì²´ì¸ ìƒíƒœ")

    next_q_btn = gr.Button("ğŸ“¢ ë‹¤ìŒ ë¬¸ì œ")
    question_output = gr.Textbox(label="ë¬¸ì œ ë³´ê¸°")
    question_audio = gr.Audio(label="ë¬¸ì œ ìŒì„±", type="filepath")

    audio_input = gr.Audio(label="ğŸ¤ ìŒì„± ë‹µë³€", type="filepath")
    result_output = gr.Textbox(label="ğŸ“Š AI í‰ê°€ ê²°ê³¼")

    pdf_input.change(fn=extract_questions, inputs=[pdf_input], outputs=[upload_result])
    load_btn.click(fn=load_vectorstore, outputs=[vectorstore_status])
    setup_btn.click(fn=setup_chain, outputs=[chain_status])
    next_q_btn.click(fn=get_next_question, outputs=[question_output, question_audio])
    audio_input.change(fn=evaluate_audio_response, inputs=[audio_input], outputs=[result_output])

demo.launch()