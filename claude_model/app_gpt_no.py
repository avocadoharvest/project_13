import gradio as gr
import os
import tempfile
from dotenv import load_dotenv
from gtts import gTTS

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

from text2speech import speech_to_text   # íŒŒì¼ ê²½ë¡œì—ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í•¨ìˆ˜ í•„ìš”

# í™˜ê²½ë³€ìˆ˜ ë° OpenAI API í‚¤ ë¡œë”©
load_dotenv()
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")

questions = []
pdf_fulltext = ""
question_index = 0
evaluate_chain = None

def extract_questions(pdf_file):
    global questions, question_index, pdf_fulltext
    if pdf_file is None:
        return "âŒ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
    pdf_path = pdf_file.name
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, separators=["\n\n", "\n", ".", "?"])
    docs = splitter.split_documents(pages)
    extracted = []
    for doc in docs:
        lines = doc.page_content.strip().split("\n")
        for line in lines:
            if line.strip().startswith("Q") and "?" in line:
                extracted.append(line.strip())
    questions = extracted
    question_index = 0
    pdf_fulltext = "\n".join([doc.page_content for doc in pages])
    return f"âœ… ì´ {len(questions)}ê°œì˜ ë¬¸ì œê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤."

def setup_chain():
    global evaluate_chain
    prompt = PromptTemplate.from_template("""
#Question: {question}
#Answer: {answer}
#Context: {context}

[ì¶œë ¥ ì˜ˆì‹œ]   
ì ìˆ˜: Bì    
í”¼ë“œë°±: ë‹µë³€ì´ í•µì‹¬ ë‚´ìš©ì„ ì˜ ì–¸ê¸‰í–ˆìœ¼ë‚˜, ì„¸ë¶€ì •ë³´ê°€ ë¹ ì§.
""")
    llm = ChatOpenAI(
        model="gpt-4o",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        temperature=0,
        streaming=False,
    )
    evaluate_chain = LLMChain(
        llm=llm,
        prompt=prompt,
    )
    return "âœ… LLM ì±„ì  ì²´ì¸ ì¤€ë¹„ ì™„ë£Œ!"

def get_next_question():
    global question_index
    if not questions:
        return "â— ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", None
    if question_index >= len(questions):
        return "ğŸ‰ ëª¨ë“  ë¬¸ì œë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!", None

    question = questions[question_index]
    question_index += 1

    tts = gTTS(text=question, lang='en')
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(tmp.name)
    tmp.close()
    return f"[ë¬¸ì œ {question_index}] {question}", tmp.name

def evaluate_audio_response(audio):
    global question_index, evaluate_chain, pdf_fulltext
    if evaluate_chain is None:
        return "âŒ í‰ê°€ ì²´ì¸ì„ ë¨¼ì € ì¤€ë¹„í•´ì£¼ì„¸ìš”."
    if question_index == 0:
        return "â— ë¬¸ì œë¥¼ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”."

    question = questions[question_index - 1]
    user_answer = speech_to_text(audio)   # ì˜¤ë””ì˜¤ íŒŒì¼ë¡œë¶€í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ

    # ì „ì²´ pdf context ì‚¬ìš© (ì—†ìœ¼ë©´ context="")
    context = pdf_fulltext if pdf_fulltext else ""

    result = evaluate_chain.invoke({
        "question": question,
        "answer": user_answer,
        "context": context,
    })
    return f"ğŸ§  AI í‰ê°€ ê²°ê³¼:\n\n{result['text'] if isinstance(result, dict) and 'text' in result else result}"

with gr.Blocks() as demo:
    gr.Markdown("## ğŸ—£ï¸ í† ìµìŠ¤í”¼í‚¹ AI ì±„ì  ì‹œìŠ¤í…œ (LLM Only ë²„ì „)")

    with gr.Row():
        pdf_input = gr.File(label="ğŸ“„ ë¬¸ì œ PDF ì—…ë¡œë“œ", file_types=[".pdf"])
        upload_result = gr.Textbox(label="ë¬¸ì œ ì¶”ì¶œ ê²°ê³¼")
    with gr.Row():
        setup_btn = gr.Button("ğŸ§  í‰ê°€ ì²´ì¸ ì¤€ë¹„")
        chain_status = gr.Textbox(label="í‰ê°€ ì²´ì¸ ìƒíƒœ")
    next_q_btn = gr.Button("ğŸ“¢ ë‹¤ìŒ ë¬¸ì œ")
    question_output = gr.Textbox(label="ë¬¸ì œ ë³´ê¸°")
    question_audio = gr.Audio(label="ë¬¸ì œ ìŒì„±", type="filepath")
    audio_input = gr.Audio(label="ğŸ¤ ìŒì„± ë‹µë³€", type="filepath")
    result_output = gr.Textbox(label="ğŸ“Š AI í‰ê°€ ê²°ê³¼")

    pdf_input.change(fn=extract_questions, inputs=[pdf_input], outputs=[upload_result])
    setup_btn.click(fn=setup_chain, outputs=[chain_status])
    next_q_btn.click(fn=get_next_question, outputs=[question_output, question_audio])
    audio_input.change(fn=evaluate_audio_response, inputs=[audio_input], outputs=[result_output])

demo.launch()
