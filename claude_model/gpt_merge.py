import gradio as gr
import os
import tempfile
from dotenv import load_dotenv
from gtts import gTTS

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from text2speech import speech_to_text  # íŒŒì¼ ê²½ë¡œì—ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í•¨ìˆ˜ í•„ìš”

# í™˜ê²½ë³€ìˆ˜ ë° OpenAI API í‚¤ ë¡œë”©
load_dotenv()
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")

questions = []
pdf_fulltext = ""
question_index = 0
llm_chain = None      # LLM only (ì „ì²´ PDF context)
vectorstore = None    # Vector DB
rag_chain = None      # RAG í‰ê°€ìš© (ì—°ê´€ context)

def extract_questions(pdf_file):
    global questions, question_index, pdf_fulltext
    if pdf_file is None:
        return "âŒ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
    pdf_path = pdf_file.name
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, separators=["\n\n", "\n", ".", "?"])
    docs = splitter.split_documents(pages)
    extracted = []
    for doc in docs:
        lines = doc.page_content.strip().split("\n")
        for line in lines:
            if line.strip().startswith("Q") and "?" in line:
                extracted.append(line.strip())
    questions = extracted
    question_index = 0
    pdf_fulltext = "\n".join([doc.page_content for doc in pages])
    return f"âœ… ì´ {len(questions)}ê°œì˜ ë¬¸ì œê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤."

def load_vectorstore():
    global vectorstore
    db_path = "./db/faiss"
    if not os.path.exists(db_path):
        return "âŒ ë²¡í„°ìŠ¤í† ì–´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    return "âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì™„ë£Œ!"

def setup_chains():
    global llm_chain, rag_chain, vectorstore
    # (1) PDF ì „ì²´ contextë¡œ í‰ê°€í•˜ëŠ” LLMì²´ì¸
    llm_prompt = PromptTemplate.from_template("""
ë„ˆëŠ” ì˜ë¬¸ ë…í•´ ì‹œí—˜ì˜ í‰ê°€ìì•¼. ì•„ë˜ Q(ë¬¸ì œ), A(ìˆ˜í—˜ìƒ ë‹µë³€), Context(PDF ì „ì²´)ê°€ ìˆì–´.

#Question: {question}
#Answer: {answer}
#Context: {context}

ìˆ˜í—˜ìƒì´ ì •ë‹µì— ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€, ë…¼ë¦¬ì  ê·¼ê±°ê°€ ì •í™•í•œì§€ 'contextë§Œ í™œìš©'í•´ì„œ í•œê¸€ë¡œ 5ë¬¸ì¥ìœ¼ë¡œ í‰ê°€, ì ìˆ˜(A~Eì , ì†Œìˆ˜ì  ê°€ëŠ¥)ë¡œ ë§¤ê²¨ì¤˜.
í”¼ë“œë°±ì€ í•œê¸€ë¡œ ë¶€íƒí•´.

[ì¶œë ¥ ì˜ˆì‹œ]
ì ìˆ˜: Bì 
í”¼ë“œë°±: ë‹µë³€ì´ í•µì‹¬ ë‚´ìš©ì„ ì˜ ì–¸ê¸‰í–ˆìœ¼ë‚˜, ì„¸ë¶€ì •ë³´ê°€ ë¹ ì§.
""")
    llm = ChatOpenAI(
        model="gpt-4o",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        temperature=0,
        streaming=False,
    )
    llm_chain = LLMChain(
        llm=llm,
        prompt=llm_prompt,
    )
    # (2) RAG-ìœ ì‚¬ë„ context(ì—°ê´€ chunk ê¸°ë°˜)
    rag_prompt = PromptTemplate.from_template("""
ë„ˆëŠ” ì˜ë¬¸ ë…í•´ ì‹œí—˜ì˜ í‰ê°€ìë‹¤. ì•„ë˜ ë¬¸ì œ, ë‹µë³€, context(ë¬¸ì„œì˜ ì¼ë¶€)ê°€ ìˆë‹¤.

#Question: {question}
#Answer: {answer}
#Context: {context}

ìˆ˜í—˜ìƒì´ ì •ë‹µì— ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€, ë…¼ë¦¬ì  ê·¼ê±°ê°€ ì •í™•í•œì§€ 'contextë§Œ í™œìš©'í•´ì„œ í•œê¸€ë¡œ 5ë¬¸ì¥ í‰ê°€Â·ì ìˆ˜(A~Eì , ì†Œìˆ˜ì  í—ˆìš©)ë¡œ ì±„ì í•˜ë¼.
[ì¶œë ¥ ì˜ˆì‹œ]
ì ìˆ˜: Bì 
í”¼ë“œë°±: ë‹µë³€ì´ í•µì‹¬ ë‚´ìš©ì„ ì˜ ì–¸ê¸‰í–ˆìœ¼ë‚˜, ì„¸ë¶€ì •ë³´ê°€ ë¹ ì§.
""")
    rag_chain = LLMChain(
        llm=llm,                 # ê°™ì€ LLM ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© (ì†ë„ìƒ ìœ ë¦¬)
        prompt=rag_prompt,
    )
    return "âœ… í‰ê°€ ì²´ì¸(LMM Only + RAG) ì¤€ë¹„ ì™„ë£Œ!"

def get_next_question():
    global question_index
    if not questions:
        return "â— ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", None
    if question_index >= len(questions):
        return "ğŸ‰ ëª¨ë“  ë¬¸ì œë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!", None

    question = questions[question_index]
    question_index += 1

    tts = gTTS(text=question, lang='en')
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(tmp.name)
    tmp.close()
    return f"[ë¬¸ì œ {question_index}] {question}", tmp.name

def evaluate_audio_response(audio):
    global question_index, llm_chain, rag_chain, vectorstore, pdf_fulltext
    if llm_chain is None or rag_chain is None:
        return "âŒ í‰ê°€ ì²´ì¸ì„ ë¨¼ì € ì¤€ë¹„í•´ì£¼ì„¸ìš”.", ""
    if question_index == 0:
        return "â— ë¬¸ì œë¥¼ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”.", ""

    question = questions[question_index - 1]
    user_answer = speech_to_text(audio)
    # (1) LLM Only: contextëŠ” PDF ì „ì²´ ë³¸ë¬¸
    context_all = pdf_fulltext if pdf_fulltext else ""
    llm_result = llm_chain.invoke({
        "question": question,
        "answer": user_answer,
        "context": context_all,
    })
    llm_feedback = f"ğŸŸ¢ [PDF ì „ì²´ context í‰ê°€]\n{llm_result['text'] if isinstance(llm_result, dict) and 'text' in llm_result else llm_result}"

    # (2) RAG: contextëŠ” vectorstoreì—ì„œ ìœ ì‚¬ chunk ì¶”ì¶œ
    if vectorstore is not None:
        docs = vectorstore.similarity_search(question, k=3)
        rag_context = "\n".join([doc.page_content for doc in docs])
        rag_result = rag_chain.invoke({
            "question": question,
            "answer": user_answer,
            "context": rag_context,
        })
        rag_feedback = f"ğŸ”µ [RAG(ì—°ê´€ chunk) í‰ê°€]\n{rag_result['text'] if isinstance(rag_result, dict) and 'text' in rag_result else rag_result}"
    else:
        rag_feedback = "â— FAISS ë²¡í„°ìŠ¤í† ì–´ê°€ ì ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    return llm_feedback, rag_feedback

with gr.Blocks() as demo:
    gr.Markdown("## ğŸ—£ï¸ í† ìµìŠ¤í”¼í‚¹ AI ì±„ì  ì‹œìŠ¤í…œ (PDF ì „ì²´/ë²¡í„° RAG ë™ì‹œ í‰ê°€)")

    with gr.Row():
        pdf_input = gr.File(label="ğŸ“„ ë¬¸ì œ PDF ì—…ë¡œë“œ", file_types=[".pdf"])
        upload_result = gr.Textbox(label="ë¬¸ì œ ì¶”ì¶œ ê²°ê³¼")
    with gr.Row():
        load_btn = gr.Button("ğŸ“‚ ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°")
        vectorstore_status = gr.Textbox(label="ë²¡í„°ìŠ¤í† ì–´ ìƒíƒœ")
    with gr.Row():
        setup_btn = gr.Button("ğŸ§  í‰ê°€ ì²´ì¸ ì¤€ë¹„")
        chain_status = gr.Textbox(label="í‰ê°€ ì²´ì¸ ìƒíƒœ")

    next_q_btn = gr.Button("ğŸ“¢ ë‹¤ìŒ ë¬¸ì œ")
    question_output = gr.Textbox(label="ë¬¸ì œ ë³´ê¸°")
    question_audio = gr.Audio(label="ë¬¸ì œ ìŒì„±", type="filepath")

    audio_input = gr.Audio(label="ğŸ¤ ìŒì„± ë‹µë³€", type="filepath")
    result_output_llm = gr.Textbox(label="ğŸ“Š [No RAG context í‰ê°€]")
    result_output_rag = gr.Textbox(label="ğŸ“Š [RAG context í‰ê°€]")

    pdf_input.change(fn=extract_questions, inputs=[pdf_input], outputs=[upload_result])
    load_btn.click(fn=load_vectorstore, outputs=[vectorstore_status])
    setup_btn.click(fn=setup_chains, outputs=[chain_status])
    next_q_btn.click(fn=get_next_question, outputs=[question_output, question_audio])
    # ì˜¤ë””ì˜¤ í‰ê°€ ê²°ê³¼ 2ê°œë¡œ ë¶„ë¦¬
    audio_input.change(
        fn=evaluate_audio_response,
        inputs=[audio_input],
        outputs=[result_output_llm, result_output_rag]
    )

demo.launch()
